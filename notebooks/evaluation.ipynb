{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "633da251",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "For this notebook we will be pulling chunk data from the PostgreSQL DB and generating responses from the backend server. \n",
    "Ensure that both containers are running and that the database is populated with your desired chunks.\n",
    "\n",
    "To start a single container instead of the entire app run `docker compose up -d <container-name>` from a terminal at the root of this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e2b43f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elena/Projects/humana/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import BleuScore, RougeScore\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import URL\n",
    "from transformers import AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db_url = URL.create(\n",
    "    drivername=\"postgresql+psycopg2\",\n",
    "    username=\"postgres\",\n",
    "    password=os.environ[\"POSTGRES_PASSWORD\"],\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    database=\"medchat\",\n",
    ")\n",
    "\n",
    "\n",
    "def get_chunks(engine):\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT chunk_id, text FROM chunks\"))\n",
    "    return result.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff9deae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 17 chunks from the database.\n",
      "\n",
      "Sample chunk data:\n",
      "Chunk ID: 1, Text: The HER-2/neu oncoFene is a member of the erbB-lik...\n",
      "Chunk ID: 2, Text: HE EVIDENCE LINKING PROTO-ONCOGENES TO THE INDUC- ...\n",
      "Chunk ID: 3, Text: More direct evidence comes from the fact that, of ...\n",
      "Chunk ID: 4, Text: Subsequent sequence analysis and chromo- somal map...\n",
      "Chunk ID: 5, Text: As a result of the published data showing amplific...\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(db_url, echo=False)\n",
    "\n",
    "chunks = get_chunks(engine)\n",
    "print(f\"Retrieved {len(chunks)} chunks from the database.\\n\")\n",
    "print(\"Sample chunk data:\")\n",
    "for chunk in chunks[:5]:\n",
    "    print(f\"Chunk ID: {chunk[0]}, Text: {chunk[1][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb5cdd",
   "metadata": {},
   "source": [
    "# Load Testset\n",
    "\n",
    "I manually generated 10 tests, each consisting of a question, sample answer, and list of relevant chunk ids. These questions are designed to cover a wide range of topics from all parts of the documents. Additionally, some questions require information from multiple chunks to answer, thereby increasing the difficulty level and testing the ability to synthesize information from different chunks.\n",
    "\n",
    "I acknowledge this testing set is still rather small, so all results should be taken lightly. However, the purpose of this notebook is not to give a definitive score for the app's performance, but rather to layout the overall testing framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85a922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is HER-2/neu?',\n",
       " 'sample_answer': 'The HER-2/neu oncogene is a member of the erbB-like oncogene family, and is related to, but distinct firom, the epidermal growth factor receptor. This gene has been shown to be amplified in human breast cancer cell lines.',\n",
       " 'chunk_ids': [1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"eval_tests.json\", \"r\") as file:\n",
    "    eval_tests = json.load(file)\n",
    "\n",
    "# show sample\n",
    "eval_tests[\"1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3cf84",
   "metadata": {},
   "source": [
    "# Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f71623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_session() -> int:\n",
    "    resp = requests.post(\n",
    "        url=\"http://localhost:5050/start_session\",\n",
    "        json={\"user_id\": 1},  # User ID 1 is the test user\n",
    "    )\n",
    "    session_id = resp.json()\n",
    "    return session_id\n",
    "\n",
    "\n",
    "def generate_response(query: str):\n",
    "\n",
    "    session_id = start_session()\n",
    "\n",
    "    resp = requests.post(\n",
    "        url=\"http://localhost:5050/chat_response\",\n",
    "        json={\n",
    "            \"query\": query,\n",
    "            \"chat_history\": \"\",\n",
    "            \"session_id\": session_id,\n",
    "        },\n",
    "        timeout=120,\n",
    "    )\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "def generate_eval_responses(test_set: dict):\n",
    "\n",
    "    for id, test in tqdm(test_set.items()):\n",
    "        query = test[\"question\"]\n",
    "        resp = generate_response(query)\n",
    "\n",
    "        chunk_ids = [chunk[\"chunk_id\"] for chunk in resp[\"context\"]]\n",
    "\n",
    "        test_set[id][\"response\"] = resp[\"response\"]\n",
    "        test_set[id][\"retrieved_chunks\"] = chunk_ids\n",
    "        test_set[id][\"message_id\"] = resp[\"message_id\"]\n",
    "    return test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:33<00:00, 15.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is HER-2/neu?',\n",
       " 'sample_answer': 'The HER-2/neu oncogene is a member of the erbB-like oncogene family, and is related to, but distinct firom, the epidermal growth factor receptor. This gene has been shown to be amplified in human breast cancer cell lines.',\n",
       " 'chunk_ids': [1],\n",
       " 'response': 'The HER-2/neu protein is an oncogene that belongs to the erbB-like family and is distinct from the epidermal growth factor receptor. It is amplified in approximately 30% of breast cancers, correlating with poorer outcomes like relapse and reduced survival. Amplification of HER-2/neu is a strong independent predictor of prognosis, often outperforming traditional markers like hormone receptor status in lymph node-positive breast cancer.',\n",
       " 'retrieved_chunks': [1, 5],\n",
       " 'message_id': 39}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = generate_eval_responses(eval_tests)\n",
    "test_set['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b8395c",
   "metadata": {},
   "source": [
    "**NOTE**: if you would like, you can now shutdown both containers to save resources. We will not be using them again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be9568",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "## Context Retrieval\n",
    "To evaluate the app's context retrieval we will first measure the precision and recall of the returned chunks for each query. \n",
    "\n",
    "As a reminder,\n",
    "\n",
    "**Recall** measures what fraction of the true (relevant) chunks were actually retrieved.\n",
    "- For example, if there are 3 relevant chunks in test and the system retrieves 2 of them, recall is 2/3 = 66%.\n",
    "- High recall ensures the system doesn’t miss important context.\n",
    "\n",
    "**Precision** measures what fraction of the retrieved chunks are actually relevant.\n",
    "- For example, if the system retrieves 2 chunks and only 1 are truly relevant, precision is 1/2 = 50%.\n",
    "- High precision ensures it doesn’t include irrelevant or distracting content.\n",
    "\n",
    "We will also measure the F1-score of the returned chunks, which is the harmonic mean of precision and recall. This is a good way to distill the recall and precision to a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca353a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is HER-2/neu?',\n",
       " 'sample_answer': 'The HER-2/neu oncogene is a member of the erbB-like oncogene family, and is related to, but distinct firom, the epidermal growth factor receptor. This gene has been shown to be amplified in human breast cancer cell lines.',\n",
       " 'chunk_ids': [1],\n",
       " 'response': 'The HER-2/neu protein is an oncogene that belongs to the erbB-like family and is distinct from the epidermal growth factor receptor. It is amplified in approximately 30% of breast cancers, correlating with poorer outcomes like relapse and reduced survival. Amplification of HER-2/neu is a strong independent predictor of prognosis, often outperforming traditional markers like hormone receptor status in lymph node-positive breast cancer.',\n",
       " 'retrieved_chunks': [1, 5],\n",
       " 'message_id': 39,\n",
       " 'recall': 1.0,\n",
       " 'precision': 0.5,\n",
       " 'f1': 0.6666666666666666}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_context_precision_recall(test_set):\n",
    "    for test_id, test_data in test_set.items():\n",
    "        relevant_chunks = set(test_data[\"chunk_ids\"])\n",
    "        retrieved_chunks = set(test_data[\"retrieved_chunks\"])\n",
    "\n",
    "        num_tp = len(relevant_chunks & retrieved_chunks)\n",
    "\n",
    "        # Compute Recall\n",
    "        if relevant_chunks:\n",
    "            recall = num_tp / len(relevant_chunks)\n",
    "        else:\n",
    "            recall = 0 \n",
    "        test_set[test_id][\"recall\"] = recall    \n",
    "\n",
    "        # Compute Precision\n",
    "        if retrieved_chunks:\n",
    "            precision = num_tp / len(retrieved_chunks)\n",
    "        else:\n",
    "            precision = 0 \n",
    "        test_set[test_id][\"precision\"] = precision\n",
    "\n",
    "        # Compute F1\n",
    "        if precision or recall:\n",
    "            f1 = 2*(precision*recall)/(precision + recall)\n",
    "        else:\n",
    "            f1 = 0\n",
    "        test_set[test_id][\"f1\"] = f1\n",
    "\n",
    "\n",
    "\n",
    "compute_context_precision_recall(test_set)\n",
    "test_set['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c62d304",
   "metadata": {},
   "source": [
    "Let's take a look at the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5651e411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.18\n",
      "Average Recall: 0.30\n",
      "Average F1 Score: 0.22\n"
     ]
    }
   ],
   "source": [
    "# Compute average over all tests\n",
    "\n",
    "avg_precision = sum(test[\"precision\"] for test in test_set.values()) / len(test_set)\n",
    "avg_recall = sum(test[\"recall\"] for test in test_set.values()) / len(test_set)\n",
    "avg_f1 = sum(test[\"f1\"] for test in test_set.values()) / len(test_set)\n",
    "\n",
    "print(f\"Average Precision: {avg_precision:.2f}\")\n",
    "print(f\"Average Recall: {avg_recall:.2f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df7088dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tests with no context retrieved: 1\n"
     ]
    }
   ],
   "source": [
    "# Count tests where no context was retrieved\n",
    "no_context_tests = sum(1 for test in test_set.values() if not test[\"retrieved_chunks\"])\n",
    "print(f\"Number of tests with no context retrieved: {no_context_tests}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bdf8eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Precision and Recall for each test\n",
    "precisions = [test[\"precision\"] for test in test_set.values()]\n",
    "recalls = [test[\"recall\"] for test in test_set.values()]\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(precisions, recalls, color='blue', alpha=0.25)\n",
    "plt.title(\"Precision vs Recall Scatter Plot\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d98de0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tests with 0 precision and recall: 7\n"
     ]
    }
   ],
   "source": [
    "zero_precision_recall_tests = sum(\n",
    "    1 for test in test_set.values() if test[\"precision\"] == 0 and test[\"recall\"] == 0\n",
    ")\n",
    "print(f\"Number of tests with 0 precision and recall: {zero_precision_recall_tests}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1751970",
   "metadata": {},
   "source": [
    "### Dicussion\n",
    "\n",
    "Overall, the results are poor, with **7 out of 10 tests scoring 0 on both precision and recall**. However, there are two important observations to highlight:\n",
    "\n",
    "1. In 2/10 cases the app did not return any chunks at all. This indicates that we may have our `MIN_CHUNK_CROSSENCODER_RELAVANCE` parameter set too high (was 5.0 during this test). \n",
    "2. In the remaining 5/10 cases, a quick inspection of the results shows that the app still gave mostly relevant responses. Therefore, it might be the case that my test set was flawed in that the answer to some questions may be contained in chunks other than the listed ones (e.g., I say the answer is contained in Chunk 6 only, when the same information can also be found in Chunk 10).\n",
    "\n",
    "To evaluate whether the test set is indeed flawed, we need to compare the similarity between the app’s response and the expected answer, which we do in the next section.\n",
    "\n",
    "## Response Quality\n",
    "\n",
    "### Traditional NLP Metrics\n",
    "\n",
    "We will first compare the responses using two traditional NLP metrics: BLEU Score and ROUGE Score.\n",
    "\n",
    "As a reminder, both metrics can be used to compare a refernce text to a candidate text by measuring the co-occurrences of n-grams. BLEU was originally developed to assess to natural language translation task and ROUGE was developed to evaluate summaries.\n",
    "\n",
    "Neither metric is particularly well suited for this task, since they only consider n-grams and not more nuanced semantic similarity. However, they are a useful baseline if nothing else.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bae8f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    SingleTurnSample(\n",
    "        response=test[\"response\"],\n",
    "        reference=test[\"sample_answer\"]\n",
    "    )\n",
    "    for test in test_set.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2683a4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.17\n"
     ]
    }
   ],
   "source": [
    "scorer = BleuScore()\n",
    "\n",
    "scores = []\n",
    "for sample in samples:\n",
    "    score = await scorer.single_turn_ascore(sample)\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Average BLEU Score: {sum(scores) / len(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44c56a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Score: 0.19\n"
     ]
    }
   ],
   "source": [
    "scorer = RougeScore()\n",
    "for sample in samples:\n",
    "    score = await scorer.single_turn_ascore(sample)\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Average ROUGE Score: {sum(scores) / len(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce553729",
   "metadata": {},
   "source": [
    "### Vector Similarity\n",
    "\n",
    "Another approach is to take the similarity between the embeddings of the given answer and the response. This should hopefully capture some of the richer semantic meaning that is missing in the BLEU and ROUGE scores.\n",
    "\n",
    "To generate the embeddings I will be using a new model, `BAAI/bge-m3`. This is a general purpose embedding model that scores well on most evaluation sets (see [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)).\n",
    "\n",
    "I am deliberately using a new family of models to avoid data leakage in our evaluations. In more detail\n",
    "- Using an embedding model from the Qwen family could skew similarity scores upward, as both the generator (Qwen3) and the embedder may have been trained on overlapping data. This could make the responses appear more semantically similar than they actually are.\n",
    "- Recall the MedCPT models were used to retrieve context that was served as input to the Qwen model during generation. Reusing MEDCPT for evaluation would effectively mean asking the same model to assess the quality of an output that it indirectly influenced. This creates a feedback loop where any semantic alignment introduced during retrieval could artificially inflate the similarity scores. It also risks conflating retrieval performance with response quality, undermining the independence of the evaluation.\n",
    "\n",
    "An additional benefit of BGE-M3 is that is that it can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval. Hence, it's basically 3 vector similarity scores in one, helping us get a more reliable overall similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87e35359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [03:31<00:00,  7.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29927cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average COLBERT Score: 0.78\n",
      "Average SPARSE Score: 0.54\n",
      "Average DENSE Score: 0.80\n",
      "Overall Average Score: 0.70\n"
     ]
    }
   ],
   "source": [
    "answers = [test[\"sample_answer\"] for test in test_set.values()]\n",
    "responses = [test[\"response\"] for test in test_set.values()]\n",
    "\n",
    "sentence_pairs = [[answer, response] for answer, response in zip(answers, responses)]\n",
    "\n",
    "scores = model.compute_score(sentence_pairs)\n",
    "\n",
    "for method in [\"colbert\", \"sparse\", \"dense\"]:\n",
    "    print(f\"Average {method.upper()} Score: {sum(scores[method]) / len(scores[method]):.2f}\")\n",
    "\n",
    "overall_avg = sum(sum(scores[method]) / len(scores[method]) for method in scores.keys()) / len(scores.keys())\n",
    "print(f\"Overall Average Score: {overall_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2fc963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ID: 3\n",
      "Question: What is the clinical significance of HER-2/neu amplification?\n",
      "Sample Answer: HER-2/neu has been shown to be amplified in human breast cancer cell lines. In the current study, alterations of the gene in 189 primary human breast cancers were investigated. HER-2/neu was found to be amplified from 2- to greater than 20-fold in 30% of the tumors. Correlation of gene amplification with several disease parameters was evaluated. Amplification of the HER-2/neu gene was a significant predictor of both overall survival and time to relapse in patients with breast cancer. It retained its significance even when adjustments were made for other known prognostic factors. Moreover, HER-2/neu amplification had greater prognostic value than most currently used prognostic factors, including hormonal-receptor status, in lymph node-positive disease.\n",
      "Response: HER2/neu amplification is associated with increased risk of cancer relapse and poorer overall survival in breast cancer patients. It is a key biomarker guiding treatment decisions, particularly for targeted therapies like trastuzumab. Amplification also influences prognosis, with higher levels correlating with more aggressive disease. Detecting HER2 status helps stratify patients for tailored therapeutic approaches.\n",
      "Overall Average Score: 0.71\n",
      "\n",
      "Test ID: 5\n",
      "Question: What factors are used for breast cancer malignancy prognosis?\n",
      "Sample Answer:  Factors that are known to be important in the prognosis of breast malignancies in individual patients include: size of the primary tumor, stage of disease at diagnosis, hormonal receptor status, and number of axillary lymph nodes involved with disease (positive nodes).\n",
      "Response: The prognosis for breast cancer is determined by factors such as tumor size and stage, lymph node involvement, histological type (e.g., ductal vs. lobular), hormone receptor status (estrogen and progesterone receptors), and HER2 expression. These factors help assess the cancer's aggressiveness, likelihood of recurrence, and response to treatment. Additional considerations include the patient's overall health and the presence of genetic mutations, though these may not always be included in initial prognosis determination.\n",
      "Overall Average Score: 0.61\n",
      "\n",
      "Test ID: 7\n",
      "Question: How is HER-2/neu amplification correlated with other predictors of breast cancer malignancies?\n",
      "Sample Answer: Of 103 tumors evaluated in the initial survey, there was essentially no correlation between gene amplification and estrogen receptor status, progesterone receptor status, size of tumors, or age at diagnosis. However, there was a significant increase in incidence of HER-2/neu gene amplification in patients with more than 3 axillary lymph nodes involved with disease. A multivariate regression analysis to correlate HER-2/neu amplification with various disease parameters identified the number of positive nodes as the only significant factor, either alone or in combination, to correlate with amplification. In a second study on 100 breast cancer samples from patients with positive axillary lymph nodes, several statistically significant or nearly significant relationships were observed. In agreement with the preliminary survey, there was an association between number of involved lymph nodes and HER-2/neu amplification. In addition, the presence of gene amplification was correlated with estrogen receptor status and size of primary tumor\n",
      "Response: HER-2/neu amplification is a significant and superior predictor of both time to relapse and overall survival in breast cancer compared to other factors like hormonal receptor status and lymph node involvement. It is associated with worse outcomes, including shorter disease-free and overall survival, particularly when amplification exceeds five copies. While the number of positive nodes is equally predictive, HER-2/neu amplification retains its significance even after adjusting for other known prognostic factors.\n",
      "Overall Average Score: 0.66\n",
      "\n",
      "Test ID: 8\n",
      "Question: Is HER-2/neu still predictive of malignancies after other prognostic factors are controlled for?\n",
      "Sample Answer: To determine if amplification of HER-2/neu was independent of other known prognostic factors in predicting disease behavior, multivariate survival analyses were performed on the 86 node-positive cases. Amplification of the gene continued to be a strong prognostic factor, providing additional and independent predictive information on both time to relapse and overall survival in these patients, even when other prognostic factors were taken into account.\n",
      "Response: Yes, HER-2/neu amplification remains a significant predictive factor for breast cancer outcomes even when other prognostic factors like hormonal receptor status and lymph node involvement are accounted for. The study found it retained statistical significance for overall survival and time to relapse, outperforming most other factors. It was particularly impactful in lymph node-positive disease, with greater copy numbers correlating to worse prognosis.\n",
      "Overall Average Score: 0.69\n",
      "\n",
      "Test ID: 9\n",
      "Question: Is HER-2/neu amplification correlated with EGFR amplification?\n",
      "Sample Answer: To determine whether the phenomenon of amplification of HER-2/neu in breast cancer extended to related growth factor receptors, all filters were analyzed with the EGFR probe. Amplification of the EGFR gene was found in 4 out of 189 (2%) of the cases, and rearrangement of the EGFR gene was found in one of those four cases. The incidence of EGFR amplification and rearrangement was too small to attempt statistical correlation. Comparison of HER-2/neu amplification (53 out of 189, or 28%) with that of the EGFR gene reveals the incidence of the former to be 14 times greater than that of the latter, indicating that the phenomenon of gene amplification is not a general one for a related tyrosine kinase-specific receptor in human breast cancer. Moreover, studies examining alterations of two other tyrosine kinase-specific proto-oncogenes, abl and fes, in breast cancer did not show amplification of these genes.\n",
      "Response: The provided context does not mention a correlation between HER-2/neu amplification and EGFR amplification in cancer patients. It focuses on HER-2/neu's relationship with survival, relapse, and other factors in breast cancer but does not address EGFR amplification or its interactions. Therefore, the answer is not contained in the given text.\n",
      "Overall Average Score: 0.61\n",
      "\n",
      "Test ID: 10\n",
      "Question: What is the clinical significance of N-myc amplification?\n",
      "Sample Answer: The N-myc gene is frequently amplified in human neuroblastomas and neuroblastoma cell lines. Studies on the N-myc proto-oncogene were the first to show a direct association between abnormalities in a proto-oncogene and clinical behavior of a human tumor. N-myc amplification and expression correlate both with stage of disease and overall survival in patients with neuroblastoma. Moreover, the greater the N-myc gene copy number, the worse the patient prognosis for all stages of the disease. Taken together, these data indicate a role for the N-myc gene in the pathogenesis of neuroblastoma.\n",
      "Response: The clinical significance of N-myc amplification lies in its association with poorer prognosis in neuroblastoma, similar to HER-2/neu amplification in breast cancer. It serves as a prognostic marker, indicating more aggressive disease and worse survival outcomes. While the context does not explicitly detail treatment implications, its role in predicting clinical outcomes underscores its importance in guiding therapeutic strategies for neuroblastoma.\n",
      "Overall Average Score: 0.63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect tests with high average scores but 0 precision and recall\n",
    "\n",
    "for test_id, test_data in test_set.items():\n",
    "    overall_avg_score = sum(scores[method][int(test_id) - 1] for method in scores.keys()) / len(scores.keys())\n",
    "    if overall_avg_score > 0.5 and test_data[\"recall\"] == 0 and test_data[\"precision\"] == 0:\n",
    "        print(f\"Test ID: {test_id}\")\n",
    "        print(f\"Question: {test_data['question']}\")\n",
    "        print(f\"Sample Answer: {test_data['sample_answer']}\")\n",
    "        print(f\"Response: {test_data['response']}\")\n",
    "        print(f\"Overall Average Score: {overall_avg_score:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82014324",
   "metadata": {},
   "source": [
    "### LLM Based Scores\n",
    "\n",
    "In addition to the metrics above, one can also use LLM-graded scores to assess the quality of responses. This involves prompting a large language model to rate or compare responses based on criteria such as correctness, relevance, completeness, or helpfulness.\n",
    "\n",
    "These methods provide several benefits, namely that:\n",
    "- LLM's can consider nuance in ways traditional scores cannot, for example sarcasm or negation\n",
    "- By changing the rubric in the prompts you can emphasize or de-emphasize specific criteria (e.g., don't say anything not fully supported by the context)\n",
    "\n",
    "However, the inherent randomness of LLM's gives me pause. Before implementing them, I would want to do a study comparing the LLM's scores to human reviewers and measuring their consistency across runs.\n",
    "\n",
    "For the purposes of this prototype, I also chose to leave them out because I don't have faith in the small models I can run on my hardware to act as reliable reviewers.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "Overall, the response quality metrics are promising! \n",
    "\n",
    "The BLEU and ROUGE scores were almost universally low, which isn't really surprising given their limitations. However the vector similarity scores are quite high, with an overall average of 0.7 across the three embedding methods.\n",
    "\n",
    "To test my hypothesis that some of the tests may be flawed, I manually inspected any tests that had an overall vector similarity > 0.5, recall = 0, and precision = 0. 6 of the 7 tests that had 0 precision and recall satisfied these conditions. \n",
    "\n",
    "In my subjective opinion, I would only classify one of these tests (test 3) as a \"False Positive\", i.e., one of the test had an irrelevant response that nonetheless scored high. For the remaining 5 tests, I found the model's answer to be relevant and agree with the sample answer, though in some cases the model's answer had less details. This supports the hypothesis that these 5 tests may be flawed, and adding additional relevant context chunks to the answers would give a better indicator of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d3664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
